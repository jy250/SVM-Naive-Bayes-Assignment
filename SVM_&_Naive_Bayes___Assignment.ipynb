{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "XvzZeZJjrrBs",
        "outputId": "f52838df-5d1c-4f05-a4f0-3b508f5b6fb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Support Vector Machine (SVM) is a supervised machine learning algorithm used mainly for classification.\\nIt works by finding the best hyperplane that separates classes of data with the maximum margin.\\nThe closest points to this hyperplane are called support vectors, which define the boundary.\\nFor non-linear data, SVM uses the kernel trick (like polynomial or RBF) to map data into higher dimensions where separation is possible.\\nA soft margin allows some misclassifications, balancing accuracy and generalization.\\nOverall, SVM is powerful for high-dimensional data but can be slow on very large or noisy datasets.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 1 What is a Support Vector Machine (SVM), and how does it work\n",
        "\"\"\"A Support Vector Machine (SVM) is a supervised machine learning algorithm used mainly for classification.\n",
        "It works by finding the best hyperplane that separates classes of data with the maximum margin.\n",
        "The closest points to this hyperplane are called support vectors, which define the boundary.\n",
        "For non-linear data, SVM uses the kernel trick (like polynomial or RBF) to map data into higher dimensions where separation is possible.\n",
        "A soft margin allows some misclassifications, balancing accuracy and generalization.\n",
        "Overall, SVM is powerful for high-dimensional data but can be slow on very large or noisy datasets.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 : Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\"\"\"Hard Margin SVM: Tries to find a hyperplane that perfectly separates the classes with no misclassification. It only works if the data is linearly separable and has no noise/outliers.\n",
        "\n",
        "Soft Margin SVM: Allows some misclassification or overlap by introducing a tolerance (slack variables). A parameter C controls the trade-off: a high C means fewer misclassifications (narrow margin), while a low C means more tolerance (wider margin, better generalization).\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "5ue9uvkosKEZ",
        "outputId": "c7aa3b74-615e-4efc-cc39-3d59b455ce90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hard Margin SVM: Tries to find a hyperplane that perfectly separates the classes with no misclassification. It only works if the data is linearly separable and has no noise/outliers.\\n\\nSoft Margin SVM: Allows some misclassification or overlap by introducing a tolerance (slack variables). A parameter C controls the trade-off: a high C means fewer misclassifications (narrow margin), while a low C means more tolerance (wider margin, better generalization).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3  What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\"\"\"The Kernel Trick in SVM is a method that allows the algorithm to handle data that is not linearly separable by mapping it into a higher-dimensional space without explicitly computing that transformation. Instead of transforming the data directly, the kernel trick computes the dot product in the higher-dimensional space using a kernel function, which is much more efficient.\n",
        "\n",
        "xample: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "The RBF kernel is defined as:\n",
        "\n",
        "ùêæ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ",\n",
        "ùë•\n",
        "ùëó\n",
        ")\n",
        "=\n",
        "exp\n",
        "‚Å°\n",
        "(\n",
        "‚àí\n",
        "ùõæ\n",
        "‚à£\n",
        "‚à£\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë•\n",
        "ùëó\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ",x\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        ")=exp(‚àíŒ≥‚à£‚à£x\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚àíx\n",
        "j\n",
        "\t‚Äã\n",
        "\n",
        "‚à£‚à£\n",
        "2\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "AKSQZ0k1sclK",
        "outputId": "4bbd0837-ee79-4b8a-b56e-088b7dbda805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Kernel Trick in SVM is a method that allows the algorithm to handle data that is not linearly separable by mapping it into a higher-dimensional space without explicitly computing that transformation. Instead of transforming the data directly, the kernel trick computes the dot product in the higher-dimensional space using a kernel function, which is much more efficient.\\n\\nxample: Radial Basis Function (RBF) Kernel\\n\\nThe RBF kernel is defined as:\\n\\nùêæ\\n(\\nùë•\\nùëñ\\n,\\nùë•\\nùëó\\n)\\n=\\nexp\\n\\u2061\\n(\\n‚àí\\nùõæ\\n‚à£\\n‚à£\\nùë•\\nùëñ\\n‚àí\\nùë•\\nùëó\\n‚à£\\n‚à£\\n2\\n)\\nK(x\\ni\\n\\t\\u200b\\n\\n,x\\nj\\n\\t\\u200b\\n\\n)=exp(‚àíŒ≥‚à£‚à£x\\ni\\n\\t\\u200b\\n\\n‚àíx\\nj\\n\\t\\u200b\\n\\n‚à£‚à£\\n2\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù\n",
        "\"\"\"A Na√Øve Bayes Classifier is a supervised machine learning algorithm based on Bayes‚Äô Theorem, which predicts the probability that a data point belongs to a certain class. It is especially popular for text classification tasks (like spam filtering, sentiment analysis).\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "VF0Zs9Dps210",
        "outputId": "5d274969-4b44-4756-e7e4-73e433be312e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Na√Øve Bayes Classifier is a supervised machine learning algorithm based on Bayes‚Äô Theorem, which predicts the probability that a data point belongs to a certain class. It is especially popular for text classification tasks (like spam filtering, sentiment analysis).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one\n",
        "\"\"\"aussian Na√Øve Bayes is used when features are continuous and assumed to follow a normal distribution (e.g., height, weight, medical data).\n",
        "\n",
        "Multinomial Na√Øve Bayes is used when features are discrete counts, such as word frequencies in text classification.\n",
        "\n",
        "Bernoulli Na√Øve Bayes is used when features are binary (0/1), indicating presence or absence of something (e.g., whether a word appears in a document).\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "GS3W3acbtK6R",
        "outputId": "59542955-420c-4e90-a8e3-53ce9044de16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aussian Na√Øve Bayes is used when features are continuous and assumed to follow a normal distribution (e.g., height, weight, medical data).\\n\\nMultinomial Na√Øve Bayes is used when features are discrete counts, such as word frequencies in text classification.\\n\\nBernoulli Na√Øve Bayes is used when features are binary (0/1), indicating presence or absence of something (e.g., whether a word appears in a document).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        " #sklearn.datasets or a CSV file you have.\n",
        " #Question 6: Write a Python program to:\n",
        "# Load the Iris dataset\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "# Print the model's accuracy and support vectors.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data   # features\n",
        "y = iris.target # labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\\n\", svm_clf.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PVDY_dbtkOp",
        "outputId": "246b2bd4-9837-429a-fef2-00a34a373830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 ‚óè Load the Breast Cancer dataset\n",
        "# Train a Gaussian Na√Øve Bayes model\n",
        "# Print its classification report including precision, recall, and F1-score\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report (precision, recall, f1-score)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmg6CXwyuIoF",
        "outputId": "d33dd30c-74fa-4031-b6f1-8a797235c65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 Train an SVM Classifier on the Wine dataset using GridSearchCV to find the bestC and gamma.\n",
        "# Print the best hyperparameters and accuracy.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']   # RBF kernel for non-linear decision boundaries\n",
        "}\n",
        "\n",
        "# Train with GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = grid.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgmRW0JKuapz",
        "outputId": "fa90d766-756f-423f-872c-00b8e3c30bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. usingsklearn.datasets.fetch_20newsgroups).\n",
        "# Print the model's ROC-AUC score for its predictions\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Load dataset\n",
        "categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# 2. Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# 3. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Train Na√Øve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict probabilities for ROC-AUC\n",
        "y_prob = nb_model.predict_proba(X_test)\n",
        "\n",
        "# 6. Binarize labels for multi-class ROC-AUC\n",
        "y_test_bin = label_binarize(y_test, classes=[0,1,2,3])\n",
        "\n",
        "# 7. Compute ROC-AUC score (macro-average)\n",
        "roc_auc = roc_auc_score(y_test_bin, y_prob, average='macro', multi_class='ovr')\n",
        "\n",
        "print(\"ROC-AUC Score (Macro, One-vs-Rest):\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuAvdifauzNw",
        "outputId": "67e73656-7389-413e-d2ab-cdd440a2ec9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score (Macro, One-vs-Rest): 0.9921642075021605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10\n",
        "\"\"\"Email Spam Detection\n",
        "\n",
        "Data & Preprocessing:\n",
        "\n",
        "Clean email text, remove missing emails.\n",
        "\n",
        "Convert text to TF-IDF vectors with unigrams and bigrams.\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "Multinomial Na√Øve Bayes: works well for sparse text data.\n",
        "\n",
        "Class Imbalance Handling:\n",
        "\n",
        "Use class_weight='balanced' (if using SVM).\n",
        "\n",
        "Oversample spam emails using SMOTE or simple duplication.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Precision, Recall, F1-Score for the spam class.\n",
        "\n",
        "ROC-AUC for model separability.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Reduce spam reaching inbox, improve user trust.\n",
        "\n",
        "Protect users from phishing/malware.\n",
        "\n",
        "Example 2: Customer Churn Prediction\n",
        "\n",
        "Data & Preprocessing:\n",
        "\n",
        "Handle missing values in demographics and usage patterns (impute with median or mode).\n",
        "\n",
        "Encode categorical variables with one-hot encoding.\n",
        "\n",
        "Standardize numerical features.\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "Random Forest Classifier: robust, handles mixed data types, interpretable feature importance.\n",
        "\n",
        "Class Imbalance Handling:\n",
        "\n",
        "Use class_weight='balanced' or oversample churned customers.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Precision/Recall/F1 for churned customers.\n",
        "\n",
        "ROC-AUC to measure overall performance.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Proactively retain customers, reducing revenue loss.\n",
        "\n",
        "Optimize marketing campaigns.\n",
        "\n",
        "Example 3: Fraud Transaction Detection\n",
        "\n",
        "Data & Preprocessing:\n",
        "\n",
        "Handle missing transaction metadata.\n",
        "\n",
        "Normalize numerical features (amount, time).\n",
        "\n",
        "Encode categorical fields like merchant type.\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "Gradient Boosting (XGBoost or LightGBM): effective for rare event detection.\n",
        "\n",
        "Class Imbalance Handling:\n",
        "\n",
        "Oversample fraud cases or set scale_pos_weight.\n",
        "\n",
        "Threshold tuning to minimize false negatives.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Recall and Precision for fraudulent transactions.\n",
        "\n",
        "ROC-AUC and PR-AUC due to extreme imbalance.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Prevent financial loss, protect customers.\n",
        "\n",
        "Reduce chargeback costs and increase trust.\n",
        "\n",
        "Example 4: Sentiment Analysis for Product Reviews\n",
        "\n",
        "Data & Preprocessing:\n",
        "\n",
        "Clean review text, remove stopwords/punctuation.\n",
        "\n",
        "Convert to TF-IDF or embeddings (e.g., Word2Vec, BERT).\n",
        "\n",
        "Model Choice:\n",
        "\n",
        "SVM with linear kernel: performs well on high-dimensional text data.\n",
        "\n",
        "Class Imbalance Handling:\n",
        "\n",
        "Weighted SVM or oversample minority sentiment class.\n",
        "\n",
        "Evaluation Metrics:\n",
        "\n",
        "Accuracy, F1-score for positive/negative classes.\n",
        "\n",
        "Confusion matrix to understand misclassification.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Understand customer sentiment to improve products.\n",
        "\n",
        "Inform marketing strategies and product improvements.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UuV8LDV9vEmo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "80b25a36-6c87-48a7-9083-b1ed365b9f84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Email Spam Detection\\n\\nData & Preprocessing:\\n\\nClean email text, remove missing emails.\\n\\nConvert text to TF-IDF vectors with unigrams and bigrams.\\n\\nModel Choice:\\n\\nMultinomial Na√Øve Bayes: works well for sparse text data.\\n\\nClass Imbalance Handling:\\n\\nUse class_weight='balanced' (if using SVM).\\n\\nOversample spam emails using SMOTE or simple duplication.\\n\\nEvaluation Metrics:\\n\\nPrecision, Recall, F1-Score for the spam class.\\n\\nROC-AUC for model separability.\\n\\nBusiness Impact:\\n\\nReduce spam reaching inbox, improve user trust.\\n\\nProtect users from phishing/malware.\\n\\nExample 2: Customer Churn Prediction\\n\\nData & Preprocessing:\\n\\nHandle missing values in demographics and usage patterns (impute with median or mode).\\n\\nEncode categorical variables with one-hot encoding.\\n\\nStandardize numerical features.\\n\\nModel Choice:\\n\\nRandom Forest Classifier: robust, handles mixed data types, interpretable feature importance.\\n\\nClass Imbalance Handling:\\n\\nUse class_weight='balanced' or oversample churned customers.\\n\\nEvaluation Metrics:\\n\\nPrecision/Recall/F1 for churned customers.\\n\\nROC-AUC to measure overall performance.\\n\\nBusiness Impact:\\n\\nProactively retain customers, reducing revenue loss.\\n\\nOptimize marketing campaigns.\\n\\nExample 3: Fraud Transaction Detection\\n\\nData & Preprocessing:\\n\\nHandle missing transaction metadata.\\n\\nNormalize numerical features (amount, time).\\n\\nEncode categorical fields like merchant type.\\n\\nModel Choice:\\n\\nGradient Boosting (XGBoost or LightGBM): effective for rare event detection.\\n\\nClass Imbalance Handling:\\n\\nOversample fraud cases or set scale_pos_weight.\\n\\nThreshold tuning to minimize false negatives.\\n\\nEvaluation Metrics:\\n\\nRecall and Precision for fraudulent transactions.\\n\\nROC-AUC and PR-AUC due to extreme imbalance.\\n\\nBusiness Impact:\\n\\nPrevent financial loss, protect customers.\\n\\nReduce chargeback costs and increase trust.\\n\\nExample 4: Sentiment Analysis for Product Reviews\\n\\nData & Preprocessing:\\n\\nClean review text, remove stopwords/punctuation.\\n\\nConvert to TF-IDF or embeddings (e.g., Word2Vec, BERT).\\n\\nModel Choice:\\n\\nSVM with linear kernel: performs well on high-dimensional text data.\\n\\nClass Imbalance Handling:\\n\\nWeighted SVM or oversample minority sentiment class.\\n\\nEvaluation Metrics:\\n\\nAccuracy, F1-score for positive/negative classes.\\n\\nConfusion matrix to understand misclassification.\\n\\nBusiness Impact:\\n\\nUnderstand customer sentiment to improve products.\\n\\nInform marketing strategies and product improvements.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQr9H73UwUtR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}